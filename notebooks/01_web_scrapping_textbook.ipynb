{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web_Scrapping Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Me salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def load_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading page {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            rows = soup.find_all('div', class_='row')\n",
    "            links = [row.find('a')['href'] for row in rows if row.find('a')]\n",
    "            return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            div = soup.find('div', class_='entry-content clearfix')\n",
    "            if div:\n",
    "                lis = div.find_all('li')\n",
    "                urls = [li.find('a')['href'] for li in lis if li.find('a')]\n",
    "                return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main = soup.find('main', id='main')\n",
    "            if main:\n",
    "                paragraphs = main.find_all('p')\n",
    "                concatenated_text = '\\n'.join(paragraph.text for paragraph in paragraphs)\n",
    "                title = soup.find('h1', class_='entry-title').text if soup.find('h1', class_='entry-title') else \"No title found\"\n",
    "                return title, concatenated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\"\n",
    "\n",
    "from tqdm import tqdm  # Make sure to import tqdm\n",
    "\n",
    "def scrape_subjects_to_dataframe(base_url, subjects):\n",
    "    data = []\n",
    "    for subject in subjects:\n",
    "        subject_url = f\"{base_url}{subject}/\"\n",
    "        print(f\"Scraping {subject.capitalize()}...\")\n",
    "        subject_links = get_subject_links(subject_url)\n",
    "        for subject_link in tqdm(subject_links, desc=f\"{subject.capitalize()} Subjects\"):\n",
    "            specific_links = get_especific_subject_links(subject_link)\n",
    "            for specific_url in tqdm(specific_links, desc=f\"{subject.capitalize()} Sub-subjects\", leave=False):\n",
    "                title, content = get_content(specific_url)\n",
    "                sub_subject = specific_url.split('/')[-2]  # Assuming the sub-subject is the second last part of the URL\n",
    "                data.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": specific_url,\n",
    "                    \"content\": content,\n",
    "                    \"subject\": subject.capitalize(),\n",
    "                    \"sub-subject\": sub_subject\n",
    "                })\n",
    "                if len(data) % 500 == 0:\n",
    "                    print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "base_url = \"https://resumos.mesalva.com/\"\n",
    "subjects = [\n",
    "    \"matematica\", \"fisica\", \"quimica\", \"biologia\", \"historia\",\n",
    "    \"geografia\", \"filosofia\", \"sociologia\", \"portugues\",\n",
    "    \"literatura\", \"artes\"\n",
    "]\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_subjects_to_dataframe(base_url, subjects)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"scraped_data.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_count\"] = df[\"content\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_count\"] = df[\"content\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kuadro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def load_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading page {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_subject_links(url):\n",
    "    try:\n",
    "        url = 'https://www.kuadro.com.br/resumos-enem-vestibulares'\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get all div class=\"Summaries__CourseRow-sc-4nmrha-0 gWDZhf\"\n",
    "            rows = soup.find_all('div', class_='Summaries__CourseRow-sc-4nmrha-0 gWDZhf')\n",
    "            # for each row, get all class=\"Summaries__Category-sc-4nmrha-3 deMeER\"\n",
    "            urls = []\n",
    "            for row in rows:\n",
    "                categories = row.find_all('a', class_='Summaries__Category-sc-4nmrha-3 deMeER')\n",
    "                urls += [category['href'] for category in categories]  \n",
    "        return urls \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            div = soup.find('div', class_='entry-content clearfix')\n",
    "            if div:\n",
    "                lis = div.find_all('li')\n",
    "                urls = [li.find('a')['href'] for li in lis if li.find('a')]\n",
    "                return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main = soup.find('main', id='main')\n",
    "            if main:\n",
    "                paragraphs = main.find_all('p')\n",
    "                concatenated_text = '\\n'.join(paragraph.text for paragraph in paragraphs)\n",
    "                title = soup.find('h1', class_='entry-title').text if soup.find('h1', class_='entry-title') else \"No title found\"\n",
    "                return title, concatenated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\"\n",
    "\n",
    "from tqdm import tqdm  # Make sure to import tqdm\n",
    "\n",
    "def scrape_subjects_to_dataframe(base_url, subjects):\n",
    "    data = []\n",
    "    for subject in subjects:\n",
    "        subject_url = f\"{base_url}{subject}/\"\n",
    "        print(f\"Scraping {subject.capitalize()}...\")\n",
    "        subject_links = get_subject_links(subject_url)\n",
    "        for subject_link in tqdm(subject_links, desc=f\"{subject.capitalize()} Subjects\"):\n",
    "            specific_links = get_especific_subject_links(subject_link)\n",
    "            for specific_url in tqdm(specific_links, desc=f\"{subject.capitalize()} Sub-subjects\", leave=False):\n",
    "                title, content = get_content(specific_url)\n",
    "                sub_subject = specific_url.split('/')[-2]  # Assuming the sub-subject is the second last part of the URL\n",
    "                data.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": specific_url,\n",
    "                    \"content\": content,\n",
    "                    \"subject\": subject.capitalize(),\n",
    "                    \"sub-subject\": sub_subject\n",
    "                })\n",
    "                if len(data) % 500 == 0:\n",
    "                    print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "base_url = \"https://resumos.mesalva.com/\"\n",
    "subjects = [\n",
    "    \"matematica\", \"fisica\", \"quimica\", \"biologia\", \"historia\",\n",
    "    \"geografia\", \"filosofia\", \"sociologia\", \"portugues\",\n",
    "    \"literatura\", \"artes\"\n",
    "]\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_subjects_to_dataframe(base_url, subjects)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "#df.to_csv(\"scraped_data.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kuadro.com.br/resumos-enem-vestibulares'\n",
    "response = load_page(url)\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # get all div class=\"Summaries__CourseRow-sc-4nmrha-0 gWDZhf\"\n",
    "    rows = soup.find_all('div', class_='Summaries__CourseRow-sc-4nmrha-0 gWDZhf')\n",
    "    # for each row, get all class=\"Summaries__Category-sc-4nmrha-3 deMeER\"\n",
    "    urls = []\n",
    "    for row in rows:\n",
    "        categories = row.find_all('a', class_='Summaries__Category-sc-4nmrha-3 deMeER')\n",
    "        urls += [category['href'] for category in categories]\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kuadro.com.br/resumos-enem-vestibulares/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = load_page(url)\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    div = soup.find('div', class_='entry-content clearfix')\n",
    "    if div:\n",
    "        lis = div.find_all('li')\n",
    "        urls = [li.find('a')['href'] for li in lis if li.find('a')]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kuadro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Ensure tqdm is installed or remove if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading page {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get all div class=\"Summaries__CourseRow-sc-4nmrha-0 gWDZhf\"\n",
    "            rows = soup.find_all('div', class_='Summaries__CourseRow-sc-4nmrha-0 gWDZhf')\n",
    "            # for each row, get all class=\"Summaries__Category-sc-4nmrha-3 deMeER\"\n",
    "            urls = []\n",
    "            for row in rows:\n",
    "                categories = row.find_all('a', class_='Summaries__Category-sc-4nmrha-3 deMeER')\n",
    "                urls += [category['href'] for category in categories]\n",
    "            return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            a = soup.find_all('a', class_='Categories__CategoryRow-sc-e5b8e1-0 vxFdC')\n",
    "            urls = [category['href'] for category in a]\n",
    "        \n",
    "            return urls \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get div resumo__Body-sc-v675m5-4 bQlNAk\n",
    "            div = soup.find('div', class_='resumo__Body-sc-v675m5-4 bQlNAk')\n",
    "            # get all text inside that div \n",
    "            text = div.text\n",
    "            title = soup.find('h1', class_='Head__Title-sc-17jjrd5-2 eUaNXA').text\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subjects_to_dataframe(base_url, resume_url):\n",
    "    data = []\n",
    "    url = f\"{base_url}{resume_url}\"\n",
    "    # subject_url = f\"{base_url}{subject}/\"\n",
    "    print(f\"Scraping {resume_url.capitalize()}...\")\n",
    "    subject_links = get_subject_links(url)\n",
    "    for subject_link in tqdm(subject_links, desc=f\"Subjects\"):\n",
    "        subject_link = f\"{base_url}/{subject_link}\"\n",
    "        specific_links = get_especific_subject_links(subject_link)\n",
    "        for specific_url in tqdm(specific_links, desc=f\"Sub-subjects\", leave=False):\n",
    "            specific_url = f\"{base_url}/{specific_url}\"\n",
    "            title, content = get_content(specific_url)\n",
    "            sub_subject = specific_url.split('/')[-1]  # Assuming the sub-subject is the second last part of the URL\n",
    "            subject = specific_url.split('/')[-3]\n",
    "            data.append({\n",
    "                \"title\": title,\n",
    "                \"url\": specific_url,\n",
    "                \"content\": content,\n",
    "                \"subject\": subject.capitalize(),\n",
    "                \"sub-subject\": sub_subject\n",
    "            })\n",
    "            # print(f\"titulo: {title}, subject: {subject.capitalize()}, sub-subject: {sub_subject}\")\n",
    "            if len(data) % 500 == 0:\n",
    "                print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "base_url = \"https://www.kuadro.com.br\"\n",
    "resume_url = \"/resumos-enem-vestibulares\"\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_subjects_to_dataframe(base_url, resume_url)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"kuadro_resume.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url has double / after .com.br remove\n",
    "# df['url'] = df['url'].str.replace('//', '/')\n",
    "df.to_csv(\"kuadro_resume.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_count\"] = df[\"content\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brasil Escola\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            a = soup.find_all('a', class_='layer-artigo--relacionado sec')\n",
    "            b = soup.find_all('a', class_='layer-artigo--relacionado')\n",
    "            urls = [category['href'] for category in a]\n",
    "            urls += [category['href'] for category in b]\n",
    "            return urls \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "url = 'https://brasilescola.uol.com.br/matematica'\n",
    "len(get_especific_subject_links(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = load_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get div resumo__Body-sc-v675m5-4 bQlNAk\n",
    "            div = soup.find('div', class_='texto-completo')\n",
    "            # get all text inside that div \n",
    "            text = div.text\n",
    "            title = soup.find('h1', class_='titulo-interna mb-4').text\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\"\n",
    "print(get_content('https://brasilescola.uol.com.br/matematica/teorema-pitagoras.htm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quero Bolsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get all div class=\"Summaries__CourseRow-sc-4nmrha-0 gWDZhf\"\n",
    "            divs = soup.find_all('div', class_='p-2')\n",
    "            # for each row, get all class=\"Summaries__Category-sc-4nmrha-3 deMeER\"\n",
    "            urls = []\n",
    "            for div in divs:\n",
    "                # get <a href...> inside the div and append\n",
    "                a = div.find('a')\n",
    "                urls.append(a['href'])\n",
    "            return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            a = soup.find_all('a', class_='layer-artigo--relacionado sec')\n",
    "            b = soup.find_all('a', class_='layer-artigo--relacionado')\n",
    "            urls = [category['href'] for category in a]\n",
    "            urls += [category['href'] for category in b]\n",
    "            return urls \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get div resumo__Body-sc-v675m5-4 bQlNAk\n",
    "            div = soup.find('div', class_='texto-completo')\n",
    "            # get all text inside that div \n",
    "            text = div.text\n",
    "            title = soup.find('h1', class_='titulo-interna mb-4').text\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subjects_to_dataframe(base_url):\n",
    "    data = []\n",
    "    url = f\"{base_url}{resume_url}\"\n",
    "    # subject_url = f\"{base_url}{subject}/\"\n",
    "    print(f\"Scraping {resume_url.capitalize()}...\")\n",
    "    subject_links = get_subject_links(url)\n",
    "    for subject_link in tqdm(subject_links, desc=f\"Subjects\"):\n",
    "        subject_link = f\"{base_url}/{subject_link}\"\n",
    "        specific_links = get_especific_subject_links(subject_link)\n",
    "        for specific_url in tqdm(specific_links, desc=f\"Sub-subjects\", leave=False):\n",
    "            specific_url = f\"{base_url}/{specific_url}\"\n",
    "            title, content = get_content(specific_url)\n",
    "            sub_subject = specific_url.split('/')[-1]  # Assuming the sub-subject is the second last part of the URL\n",
    "            subject = specific_url.split('/')[-3]\n",
    "            data.append({\n",
    "                \"title\": title,\n",
    "                \"url\": specific_url,\n",
    "                \"content\": content,\n",
    "                \"subject\": subject.capitalize(),\n",
    "                \"sub-subject\": sub_subject\n",
    "            })\n",
    "            # print(f\"titulo: {title}, subject: {subject.capitalize()}, sub-subject: {sub_subject}\")\n",
    "            if len(data) % 500 == 0:\n",
    "                print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "base_url = \"https://www.kuadro.com.br\"\n",
    "resume_url = \"/resumos-enem-vestibulares\"\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_subjects_to_dataframe(base_url, resume_url)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"kuadro_resume.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_count\"] = df[\"content\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quero Bolsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get div resumo__Body-sc-v675m5-4 bQlNAk\n",
    "            sections = soup.find_all('section')\n",
    "            # get all text in each section and then concat all \n",
    "            title = soup.find('h1').text\n",
    "            text = '\\n'.join(section.text for section in sections[:-1])\n",
    "            return text, title\n",
    "\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Ensure tqdm is installed or remove if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading page {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get all div class=\"Summaries__CourseRow-sc-4nmrha-0 gWDZhf\"\n",
    "            divs = soup.find_all('div', class_='z-card categories__card js-category-card z-card--shadow-high')\n",
    "            # for each row, get all class=\"Summaries__Category-sc-4nmrha-3 deMeER\"\n",
    "            urls = []\n",
    "            for div in divs:\n",
    "                # get <a href...> inside the div and append\n",
    "                a = div.find('a')\n",
    "                urls.append(a['href'])\n",
    "            return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_especific_subject_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            aas = soup.find_all('a', class_='z-link category__lessons-link')\n",
    "            urls = [category['href'] for category in aas]\n",
    "            return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # get div resumo__Body-sc-v675m5-4 bQlNAk\n",
    "            sections = soup.find_all('section')\n",
    "            # get all text in each section and then concat all \n",
    "            title = soup.find('h1').text\n",
    "            text = '\\n'.join(section.text for section in sections[:-1])\n",
    "            return title, text\n",
    "\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subjects_to_dataframe(base_url, resume_url):\n",
    "    data = []\n",
    "    url = f\"{base_url}{resume_url}\"\n",
    "    # subject_url = f\"{base_url}{subject}/\"\n",
    "    print(f\"Scraping {resume_url.capitalize()}...\")\n",
    "    subject_links = get_subject_links(url)\n",
    "    for subject_link in tqdm(subject_links, desc=f\"Subjects\"):\n",
    "        subject_link = f\"{base_url}{subject_link}\"\n",
    "        specific_links = get_especific_subject_links(subject_link)\n",
    "        for specific_url in tqdm(specific_links, desc=f\"Sub-subjects\", leave=False):\n",
    "            specific_url = f\"{base_url}{specific_url}\"\n",
    "            title, content = get_content(specific_url)\n",
    "            sub_subject = specific_url.split('/')[-1]  # Assuming the sub-subject is the second last part of the URL\n",
    "            subject = specific_url.split('/')[-2]\n",
    "            data.append({\n",
    "                \"title\": title,\n",
    "                \"url\": specific_url,\n",
    "                \"content\": content,\n",
    "                \"subject\": subject.capitalize(),\n",
    "                \"sub-subject\": sub_subject\n",
    "            })\n",
    "            # print(f\"titulo: {title}, subject: {subject.capitalize()}, sub-subject: {sub_subject}\")\n",
    "            # print(f\"c: {content}\")\n",
    "            if len(data) % 500 == 0:\n",
    "                print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "base_url = \"https://querobolsa.com.br\"\n",
    "resume_url = \"/enem/manual-do-enem\"\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_subjects_to_dataframe(base_url, resume_url)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"quero_bolsa.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_count\"] = df[\"content\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brasil Escola Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Ensure tqdm is installed or remove if not needed\n",
    "def load_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error loading page {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_especific_subject_questions_links(url):\n",
    "    try:\n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            divs = soup.find_all('div', class_='single-exerc')\n",
    "            links = [div.find('a')['href'] for div in divs if div.find('a')]\n",
    "            return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting specific subject links from {url}: {e}\")\n",
    "    return []\n",
    "\n",
    "def get_questions(url):\n",
    "    try:\n",
    "        subject = url.split('/')[-2]\n",
    "        \n",
    "        response = load_page(url)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            questions = soup.find_all('div', class_='question-box')\n",
    "            title = soup.find('h1').text\n",
    "            questions_list = []\n",
    "            for question in questions:\n",
    "                try: \n",
    "                    # question text is in all p in a div question-text concat then with \\n\n",
    "                    question_text = '\\n'.join(p.text for p in question.find('div', class_='question-text').find_all('p'))\n",
    "                    answer_text = question.find('div', class_='answer-text').text\n",
    "                    answer_item = answer_text[7] if answer_text[7] in ['A', 'B', 'C', 'D', 'E'] else \"\"\n",
    "                    \n",
    "                    questions_list.append({\n",
    "                        \"question\": question_text,\n",
    "                        \"answer\": answer_item,\n",
    "                        \"answer_text\": answer_text,\n",
    "                        \"subject\": subject,\n",
    "                        \"title\": title\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting question\")\n",
    "                    \n",
    "                \n",
    "            return questions_list\n",
    "\n",
    "        # return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting content from {url}: {e}\")\n",
    "    return \"No title found\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_questions_to_dataframe(subjects_links):\n",
    "    data = []\n",
    "    \n",
    "    \n",
    "    print(f\"Scraping {subjects_links[0].split('/')[2]}...\")\n",
    "    \n",
    "    for subject_link in tqdm(subjects_links, desc=f\"Subjects\"):\n",
    "        \n",
    "        specific_links = get_especific_subject_questions_links(subject_link)\n",
    "        for specific_url in tqdm(specific_links, desc=f\"Sub-subjects\", leave=False):\n",
    "            \n",
    "            questions = get_questions(specific_url)\n",
    "            data += questions\n",
    "\n",
    "            if len(data) % 500 == 0:\n",
    "                print(f\"Scraped {len(data)} pages\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define the base URL and subjects\n",
    "subjects_list = [\"exercicios-geografia\", \"exercicios-geografia-do-brasil\",\"exercicios-historia-do-brasil\",\"exercicios-historia\",\"exercicios-historia-da-america\", \"exercicios-literatura\", \"exercicios-redacao\", \"exercicios-gramatica\", \"exercicios-biologia\", \"exercicios-fisica\", \"exercicios-matematica\", \"exercicios-quimica\", \"exercicios-sociologia\", \"exercicios-ingles\", \"exercicios-filosofia\"] \n",
    "subjects_links = [f\"https://exercicios.brasilescola.uol.com.br/{subject}\" for subject in subjects_list]\n",
    "\n",
    "# Scrape data and create DataFrame\n",
    "df = scrape_questions_to_dataframe(subjects_links)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"brasil_escola_questions.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a beautiful hihstogram by subject, i split all subjects by - and remove the first word\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df['subject_1'] = df['subject'].apply(lambda x: ' '.join(x.split('-')[1:2]))\n",
    "plt.figure(figsize=(10, 6))\n",
    "# do a lateral plot because the names are too big\n",
    "sns.histplot(df, x='subject_1', kde=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "# sns.histplot(df, x='subject', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = get_questions('https://exercicios.brasilescola.uol.com.br/exercicios-geografia/enem-lista-de-exercicios-sobre-crescimento-populacional-e-teorias-demograficas.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")# df.to_csv(\"scraped_data.csv\", index=False)\n",
    "df[\"token_question\"] = df[\"question\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "df[\"token_answer\"] = df[\"answer_text\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = df['token_question'].sum() + df['token_answer'].sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
